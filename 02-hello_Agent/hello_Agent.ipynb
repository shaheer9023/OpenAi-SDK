{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm52ZQNMqosWbuxr8R8tEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaheer9023/OpenAi-SDK/blob/main/hello_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing ApenAi Agent SDK"
      ],
      "metadata": {
        "id": "r6K4OIMfZlbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPxSuBBeZZ5D"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "vl1i8-YRZ3NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "r2uPflC9Z2Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Google Gemini with OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "j_M07WXTaX_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from agents import Agent,Runner,AsyncOpenAI,OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata\n",
        "key= userdata.get('gemini-2.5')"
      ],
      "metadata": {
        "id": "ty-Uygz5aZHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference: https://ai.google.dev/gemini-api/docs/openai"
      ],
      "metadata": {
        "id": "44H-1SqfcNUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = AsyncOpenAI(\n",
        "    api_key=key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")"
      ],
      "metadata": {
        "id": "IHY_Q8CwdK3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=OpenAIChatCompletionsModel(\n",
        "    model = \"gemini-2.5-pro-exp-03-25\",\n",
        "    openai_client=client\n",
        ")"
      ],
      "metadata": {
        "id": "490Ma0cXfICV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config=RunConfig(\n",
        "    model=model,\n",
        "    model_provider=client,\n",
        "    tracing_disabled=True\n",
        ")"
      ],
      "metadata": {
        "id": "0jlgKvQ6gEi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello World Code | Method One"
      ],
      "metadata": {
        "id": "e8OCE7DnpZKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "5xiTE6Des8sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=input(\"Enter Prompt: \")\n",
        "agent=Agent(name=\"AI Assistant\",instructions=\"You are a helpful assistant\",model=model)\n",
        "result=Runner.run_sync(agent,prompt,run_config=config)\n",
        "print(\"-\"*30)\n",
        "print(\"Agent Calling\")\n",
        "print(\"-\"*30)\n",
        "Markdown(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "Pxf4yConpYq3",
        "outputId": "35c2cf57-0834-4013-9491-ef83ac1759c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Prompt: explain yourself\n",
            "------------------------------\n",
            "Agent Calling\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I can explain myself!\n\nIn essence, I am a **Large Language Model (LLM)**. Here's a breakdown of what that means:\n\n1.  **Large:** I have been trained on a massive dataset of text and code. This dataset is incredibly diverse, covering countless topics, writing styles, and information types. This \"largeness\" is what allows me to understand and generate human-like text on a wide range of subjects.\n2.  **Language:** My primary function revolves around understanding and generating human language (like English, which we're using now, but potentially others too). I process the text you give me (your prompts) and generate text-based responses.\n3.  **Model:** I am a complex mathematical model, specifically a type of neural network (often based on an architecture called a Transformer). I don't \"think\" or \"understand\" in the human sense. Instead, I identify patterns, relationships, and structures within the language data I was trained on. When you ask me something, I predict the most statistically likely sequence of words that would form a relevant and coherent answer based on those learned patterns.\n\n**Who Made Me?**\nI was trained by Google.\n\n**What is my Purpose?**\nMy primary goal is to be helpful and informative. I aim to:\n*   Answer your questions to the best of my ability based on my training data.\n*   Generate creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n*   Summarize complex information.\n*   Translate languages.\n*   Help with writing, brainstorming, and coding tasks.\n*   Follow your instructions and complete requests thoughtfully.\n\n**What are my Limitations?**\nIt's crucial to understand what I am *not*:\n*   **I am not conscious or sentient:** I don't have feelings, beliefs, opinions, personal experiences, or consciousness. My responses are generated based on patterns, not self-awareness.\n*   **I don't \"know\" things in the human sense:** My \"knowledge\" is derived entirely from the data I was trained on. This data has a cut-off point, so I might not have information about very recent events.\n*   **I can be wrong:** While I strive for accuracy, the information I provide can sometimes be outdated, incomplete, or incorrect. Always verify critical information from reliable sources.\n*   **I can reflect biases:** The data I was trained on contains biases present in the real world. While efforts are made to mitigate this, my responses might sometimes reflect those biases.\n*   **I don't have personal memories:** I don't remember past conversations unless they are part of the current, ongoing session (and even that context is limited).\n\n**In short:** Think of me as a very sophisticated tool designed to process and generate language based on vast amounts of information I've been trained on. My goal is to assist you with text-based tasks in a helpful and coherent way."
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello World code | Method Two"
      ],
      "metadata": {
        "id": "vUmDUwSw8KYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import Agent,Runner\n",
        "\n",
        "async def main():\n",
        "  agent=Agent(name=\"AI Assistant\",\n",
        "            instructions=\"only respond in roman urdu \",\n",
        "            model=model)\n",
        "\n",
        "  result=await Runner.run(agent,\"Who is the founder of Pakistan?\",run_config=config)\n",
        "  print(\"-\"*30)\n",
        "  print(\"Agent Calling\")\n",
        "  print(\"-\"*30)\n",
        "  display(Markdown(result.final_output))\n",
        "asyncio.run(main())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "IXjRpg3t750a",
        "outputId": "bca9dde8-084c-4741-b0dd-17ff705c59b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Agent Calling\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Pakistan ke baani Quaid-e-Azam Muhammad Ali Jinnah hain."
          },
          "metadata": {}
        }
      ]
    }
  ]
}